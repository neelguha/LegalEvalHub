{
  "task_id": "ssla_plaintiff",
  "name": "ssla_plaintiff",
  "family": "LegalBench",
  "short_description": "Extract plaintiff identities from securities class action complaint excerpts.",
  "long_description": "The 'ssla_plaintiff' task evaluates the ability of legal AI systems to accurately extract the identities of plaintiffs from excerpts of securities class action complaints. This task measures the AI's capability to interpret legal texts, specifically focusing on identifying named parties involved in litigation. The task is critical as it tests the AI's understanding of legal documents and its ability to discern relevant information amidst potentially complex legal language and structures. \n\nLegal reasoning in this task involves interpretation, as the AI must analyze the context of the excerpts to determine whether the plaintiff's identity is explicitly mentioned or if it should return 'Not named' when the information is absent. This task is significant for evaluating legal AI systems because accurate identification of parties in legal documents is foundational for various legal analyses, including case law research, litigation strategy, and compliance assessments. Understanding who the plaintiffs are in securities class actions is particularly important, as these cases often involve significant financial implications and regulatory scrutiny.\n\nThe task is rooted in the principles of securities law, which governs transactions and trading of securities, and is vital for maintaining market integrity and protecting investors. By ensuring that AI systems can correctly extract and identify plaintiffs, we can enhance the efficiency and reliability of legal research and practice in the realm of securities litigation.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 1036,
  "tags": [
    "LegalBench",
    "interpretation",
    "plaintiff identification",
    "rule application",
    "securities law"
  ],
  "document_type": "legal text",
  "min_input_length": 16,
  "max_input_length": 1710,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[SSLA](https://sla.law.stanford.edu/)",
  "license": "[CC by 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Extraction"
}