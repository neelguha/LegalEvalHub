{
  "task_id": "sara_numeric",
  "name": "statutory_reasoning_assessment",
  "family": "LegalBench",
  "short_description": "Evaluates tax calculation based on statutes and case facts in US federal tax law.",
  "long_description": "The SARA Numeric task assesses the ability to interpret and apply US federal tax law by determining the tax owed by an individual based on a given set of facts and relevant statutes. This task specifically measures the legal capability to analyze numerical questions that arise from statutory provisions, requiring a deep understanding of tax regulations and their implications in various scenarios. Participants must synthesize information from both the statute and the case facts to arrive at a precise monetary answer, demonstrating their proficiency in legal reasoning and numerical analysis within the context of tax law.\n\nThis task is crucial for evaluating legal AI systems as it tests their ability to navigate complex legal texts and apply legal principles to real-world situations. The ability to accurately compute tax obligations is fundamental for legal practitioners, tax professionals, and AI systems designed to assist in legal decision-making. By focusing on numerical questions derived from statutory reasoning, the SARA Numeric task provides insights into the effectiveness of AI in understanding and applying legal frameworks, which is essential for ensuring compliance and informed decision-making in tax-related matters.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 100,
  "tags": [
    "LegalBench",
    "interpretation",
    "rule application",
    "statutory interpretation",
    "tax law"
  ],
  "document_type": "statute",
  "min_input_length": 8888,
  "max_input_length": 9165,
  "metrics": [
    {
      "name": "mean_square_error",
      "direction": "minimize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "<https://nlp.jhu.edu/law/>",
  "license": "MIT",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}