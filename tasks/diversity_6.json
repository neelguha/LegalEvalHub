{
  "task_id": "diversity_6",
  "name": "diversity_6",
  "family": "LegalBench",
  "short_description": "Evaluates understanding of diversity jurisdiction criteria in legal claims.",
  "long_description": "The 'diversity_6' task measures the legal capability to assess whether the criteria for diversity jurisdiction are satisfied in a given set of facts. Specifically, it evaluates the understanding of complete diversity between plaintiffs and defendants, as well as the amount-in-controversy requirement, which must exceed $75,000. Participants must analyze fact patterns involving two plaintiffs and two defendants, each asserting multiple claims, to determine if the legal standards for federal jurisdiction are met.\n\nThis task requires rule application and legal reasoning to correctly interpret the nuances of diversity jurisdiction. Participants must discern whether the citizenships of the parties involved meet the complete diversity requirement and whether the aggregated claims exceed the stipulated amount-in-controversy. The importance of this task lies in its ability to evaluate legal AI systems' proficiency in applying complex jurisdictional rules, which are critical for determining the appropriate venue for legal disputes. Understanding these concepts is fundamental for practitioners in navigating federal and state court systems effectively.\n\nDiversity jurisdiction is a key principle in U.S. federal law, allowing federal courts to hear cases involving state law when parties are from different states. This task is particularly relevant in contexts where jurisdictional issues can significantly impact the outcome of legal proceedings, making it essential for legal AI systems to demonstrate accuracy in such evaluations.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 306,
  "tags": [
    "LegalBench",
    "diversity jurisdiction",
    "jurisdiction",
    "rule application",
    "rule conclusion"
  ],
  "document_type": "fact pattern",
  "min_input_length": 85,
  "max_input_length": 108,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "Neel Guha",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Rule-application/Rule-conclusion",
  "task_type": "Binary classification"
}