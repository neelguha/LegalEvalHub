{
  "task_id": "personal_jurisdiction",
  "name": "personal_jurisdiction",
  "family": "LegalBench",
  "short_description": "Evaluates the determination of personal jurisdiction in legal disputes.",
  "long_description": "The 'personal_jurisdiction' task measures the ability to apply legal principles governing personal jurisdiction, specifically in the context of federal law. This task requires legal reasoning that involves analyzing a set of facts to determine whether a court has the authority to exercise jurisdiction over a defendant based on their contacts with the forum state or their domicile. Participants must assess whether the defendant's actions are sufficient to establish a connection with the forum and whether the claims arise from that connection, which are critical components of personal jurisdiction analysis.\n\nThis task is vital for evaluating legal AI systems because personal jurisdiction is a foundational aspect of civil procedure that affects the fairness and efficiency of legal proceedings. Understanding jurisdictional issues is crucial for practitioners, as improper jurisdiction can lead to dismissals or unfavorable outcomes for clients. The task focuses on simplified fact patterns to test the application of jurisdictional rules, providing insights into how well AI systems can interpret and apply complex legal doctrines in practical scenarios. By honing in on the nuances of domicile and sufficient contacts, this benchmark helps ensure that AI tools can assist legal professionals in navigating jurisdictional challenges effectively.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 54,
  "tags": [
    "LegalBench",
    "civil procedure",
    "jurisdiction",
    "rule application",
    "rule conclusion"
  ],
  "document_type": "fact pattern",
  "min_input_length": 71,
  "max_input_length": 139,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "Neel Guha",
  "license": "[CC by 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Rule-application/Rule-conclusion",
  "task_type": "Binary classification"
}