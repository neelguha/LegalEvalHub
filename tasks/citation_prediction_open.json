{
  "task_id": "citation_prediction_open",
  "name": "citation_prediction_open",
  "family": "LegalBench",
  "short_description": "Predict the best case citation supporting a given legal statement.",
  "long_description": "The 'citation_prediction_open' task evaluates the ability of legal AI systems to identify the most appropriate case citation that supports a specific legal statement. This task measures the AI's capability to recall relevant legal precedents, which is a crucial aspect of legal reasoning. By focusing on sentences from federal circuit court opinions that are directly cited, the task emphasizes the importance of precise legal knowledge and the ability to connect legal principles with authoritative sources. The task is framed around the concept of 'best' citation, which refers to the case a court has chosen to support its legal reasoning, thereby testing the AI's understanding of judicial hierarchy and citation norms.\n\nThis task requires a nuanced understanding of legal reasoning, particularly the ability to analyze a statement and determine which case law best aligns with the legal principles expressed. Legal practitioners often rely on case law to substantiate their arguments, making this task vital for assessing the effectiveness of AI systems in legal research and citation practices. By evaluating how well an AI can predict case citations, stakeholders can better understand its potential applications in legal education, practice, and research, ultimately enhancing the efficiency and accuracy of legal work. The task draws on recent circuit court opinions, ensuring relevance to current legal standards and practices.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 55,
  "tags": [
    "LegalBench",
    "case law",
    "judicial opinion",
    "rule application",
    "rule-recall"
  ],
  "document_type": "judicial opinion",
  "min_input_length": 20,
  "max_input_length": 95,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "Michael Livermore, Daniel N. Rockmore",
  "license": "[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Rule-recall",
  "task_type": "Open generation"
}