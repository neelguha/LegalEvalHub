{
  "task_id": "supply_chain_disclosure_best_practice_accountability",
  "name": "supply_chain_disclosure_best_practice_accountability",
  "family": "LegalBench",
  "short_description": "Evaluates compliance disclosures on human trafficking and slavery accountability.",
  "long_description": "This task measures the legal capability to interpret supply chain disclosures specifically regarding internal compliance procedures related to human trafficking and slavery. It requires the evaluator to determine whether the retail seller or manufacturer has established any internal accountability mechanisms that align with company standards on these critical issues. The task emphasizes the importance of understanding the nuances of compliance statements, particularly in the context of legal obligations surrounding human rights and ethical business practices.\n\nThe reasoning required for this task involves careful interpretation of the language used in supply chain disclosures. Evaluators must discern whether the disclosures explicitly mention the existence of internal compliance procedures, as mere references to compliance with laws or requests for documentary evidence do not suffice. This task is crucial for assessing legal AI systems because it reflects the ability to analyze and classify complex legal texts, ensuring that AI can support businesses in adhering to ethical standards and legal requirements in their supply chains. The legal concepts involved include corporate accountability, compliance with human rights laws, and the ethical implications of supply chain management.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 387,
  "tags": [
    "LegalBench",
    "corporate law",
    "human trafficking",
    "interpretation",
    "rule application"
  ],
  "document_type": "disclosure statement",
  "min_input_length": 107,
  "max_input_length": 5764,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "Adam Chilton & Galit Sarfaty",
  "license": "[CC by 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}