{
  "task_id": "cuad_covenant_not_to_sue",
  "name": "cuad_covenant_not_to_sue",
  "family": "LegalBench",
  "short_description": "Evaluates if a clause restricts contesting ownership of intellectual property.",
  "long_description": "The 'cuad_covenant_not_to_sue' task measures the legal capability to identify and classify contractual clauses that impose restrictions on a party's ability to contest the validity of the counterparty's ownership of intellectual property or to bring claims unrelated to the contract. This task specifically assesses the understanding of covenants not to sue, which are critical in contract law as they delineate the boundaries of legal recourse available to the parties involved. By determining whether a clause qualifies as a covenant not to sue, the task evaluates the model's ability to interpret nuanced legal language within contracts.\n\nThis task requires a sophisticated level of legal reasoning and analysis, as it involves interpreting the language of contractual clauses to ascertain their implications on the parties' rights and obligations. Legal practitioners must be adept at recognizing how such clauses can impact the enforceability of claims and the overall legal strategy in disputes. Evaluating AI systems through this task is vital, as it helps ensure that legal technology can accurately interpret and apply complex legal concepts, which is essential for effective contract review and risk management in legal practice. Understanding covenants not to sue is crucial for parties entering into contracts, as these provisions can significantly limit future litigation options and influence the negotiation dynamics between parties.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 314,
  "tags": [
    "LegalBench",
    "contract law",
    "intellectual property",
    "interpretation",
    "rule application"
  ],
  "document_type": "contract clause",
  "min_input_length": 15,
  "max_input_length": 628,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/cuad>)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}