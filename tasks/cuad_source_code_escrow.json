{
  "task_id": "cuad_source_code_escrow",
  "name": "cuad_source_code_escrow",
  "family": "LegalBench",
  "short_description": "Classifies clauses requiring source code escrow in contracts.",
  "long_description": "The 'cuad_source_code_escrow' task evaluates the ability to identify contractual clauses that mandate one party to deposit its source code into escrow with a third party. This task specifically measures the legal capability to interpret and analyze the implications of source code escrow agreements, which are crucial in protecting intellectual property rights and ensuring business continuity in the event of a party's bankruptcy or insolvency. Understanding these clauses is essential for legal practitioners who draft, review, or negotiate technology-related contracts, as they can significantly affect the rights and obligations of the parties involved.\n\nThe reasoning required for this task involves careful interpretation of the language used in contractual clauses, necessitating a nuanced understanding of legal terminology and the context in which these clauses operate. Legal AI systems must demonstrate proficiency in discerning whether a clause meets the criteria for source code escrow, which is vital for assessing the risk management strategies of technology companies. This task is important for evaluating legal AI systems because it reflects their capability to perform tasks that require specialized legal knowledge and reasoning, ultimately contributing to more efficient contract management and compliance in the tech industry.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 124,
  "tags": [
    "LegalBench",
    "contract law",
    "intellectual property",
    "interpretation",
    "rule application"
  ],
  "document_type": "contract clause",
  "min_input_length": 18,
  "max_input_length": 780,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/cuad>)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}