{
  "task_id": "contract_nli_survival_of_obligations",
  "name": "contract_nli_survival_of_obligations",
  "family": "LegalBench",
  "short_description": "Evaluates if contract obligations survive termination of the agreement.",
  "long_description": "The 'contract_nli_survival_of_obligations' task measures the ability to interpret specific clauses within contracts, particularly focusing on whether certain obligations continue to exist after the termination of an agreement. This task is crucial for understanding the implications of contract language, especially in non-disclosure agreements (NDAs), where the survival of obligations can significantly affect the parties involved. Legal professionals must be adept at identifying these nuances to ensure compliance and mitigate risks associated with contractual obligations.\n\nThis task requires legal reasoning and analysis skills, particularly in the area of interpretation. Participants must determine if a given clause explicitly states that some obligations will survive the termination of the agreement, which involves discerning the intent and implications of the contractual language used. The importance of this task in evaluating legal AI systems lies in its ability to assess how well AI can understand and apply legal concepts, which is essential for automating contract analysis and ensuring that AI systems can support legal practitioners effectively. The background legal concepts involved include contract law principles, particularly those related to the enforceability of obligations post-termination, which are vital for maintaining the integrity of contractual relationships.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 165,
  "tags": [
    "LegalBench",
    "contract law",
    "interpretation",
    "obligations"
  ],
  "document_type": "contract clause",
  "min_input_length": 22,
  "max_input_length": 348,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[ContractNLI](https://stanfordnlp.github.io/contract-nli/)",
  "license": "[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}