{
  "task_id": "maud_knowledge_definition",
  "name": "maud_knowledge_definition",
  "family": "LegalBench",
  "short_description": "Evaluates understanding of knowledge definitions in merger agreements.",
  "long_description": "The 'maud_knowledge_definition' task measures the ability to interpret and classify legal knowledge concepts as they pertain to merger agreements. Specifically, it focuses on distinguishing between types of knowledge, such as 'actual knowledge' and 'constructive knowledge', within the context of legal documents. This task requires nuanced legal reasoning and analysis, as it involves understanding how these definitions impact the obligations and liabilities of parties involved in a merger.\n\nThis task is crucial for evaluating legal AI systems because it tests their capability to comprehend and apply legal definitions accurately, which is fundamental in legal practice. The ability to interpret such terms correctly can significantly influence the outcome of legal negotiations and disputes. The task is grounded in the complexities of merger agreements, which are intricate legal documents that require precise understanding of various legal concepts to ensure compliance and mitigate risks associated with corporate transactions.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 168,
  "tags": [
    "LegalBench",
    "corporate law",
    "interpretation",
    "knowledge definition"
  ],
  "document_type": "merger agreement",
  "min_input_length": 11,
  "max_input_length": 1691,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}