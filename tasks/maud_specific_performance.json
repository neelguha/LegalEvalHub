{
  "task_id": "maud_specific_performance",
  "name": "maud_specific_performance",
  "family": "LegalBench",
  "short_description": "Evaluates understanding of Specific Performance clauses in merger agreements.",
  "long_description": "The 'maud_specific_performance' task measures the ability to interpret and analyze Specific Performance clauses within merger agreements. This task specifically assesses the legal knowledge surrounding contractual obligations and the remedies available in the event of a breach. Participants must demonstrate their understanding of the precise wording and implications of these clauses, which are critical in determining the rights and entitlements of parties involved in a merger or acquisition when faced with non-compliance by one party.\n\nThis task requires legal reasoning and interpretation skills, as it involves selecting the most appropriate characterization of a Specific Performance clause from a set of options based on an excerpt from a merger agreement. Understanding these clauses is vital for evaluating legal AI systems because they play a significant role in corporate law and transactional practice, influencing how parties can enforce their rights. The ability to accurately identify and interpret such legal language is essential for ensuring that AI systems can assist legal professionals effectively in real-world scenarios.\n\nThe task is grounded in the principles of contract law, particularly focusing on the enforceability of agreements and the legal remedies available for breaches. Specific Performance is a remedy that compels a party to fulfill their contractual obligations, making it a crucial concept in merger agreements where parties often seek assurance of compliance to protect their interests.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 179,
  "tags": [
    "LegalBench",
    "corporate law",
    "interpretation",
    "specific performance"
  ],
  "document_type": "merger agreement",
  "min_input_length": 38,
  "max_input_length": 400,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}