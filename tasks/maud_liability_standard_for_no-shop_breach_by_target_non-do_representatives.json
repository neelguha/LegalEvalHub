{
  "task_id": "maud_liability_standard_for_no-shop_breach_by_target_non-do_representatives",
  "name": "maud_liability_standard_for_no-shop_breach_by_target_non-d&o_representatives",
  "family": "LegalBench",
  "short_description": "Evaluates the liability standard for no-shop breaches in merger agreements.",
  "long_description": "This task measures the legal capability to interpret and analyze liability standards specifically related to no-shop provisions in merger agreements. It focuses on understanding the nuances of contractual language and the implications of different liability standards, such as reasonable standard versus strict liability, as they apply to Target Non-D&O Representatives. Legal professionals must possess a strong grasp of contract law and the specific terms used in merger agreements to accurately determine the liability standard in question.\n\nThe task requires legal reasoning that involves careful interpretation of contractual excerpts and the ability to apply legal knowledge to select the correct liability standard from provided options. This is crucial for evaluating legal AI systems, as it tests their ability to comprehend complex legal texts and make informed decisions based on that understanding. The concepts involved are fundamental to corporate law, particularly in the context of mergers and acquisitions, where the terms of agreements can significantly impact the responsibilities and potential liabilities of the parties involved.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 157,
  "tags": [
    "LegalBench",
    "corporate law",
    "interpretation",
    "merger agreement",
    "rule application"
  ],
  "document_type": "merger agreement",
  "min_input_length": 35,
  "max_input_length": 248,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}