{
  "task_id": "cuad_post-termination_services",
  "name": "cuad_post-termination_services",
  "family": "LegalBench",
  "short_description": "Classifies contractual clauses for post-termination obligations.",
  "long_description": "The 'cuad_post-termination_services' task evaluates the ability to identify and classify contractual clauses that impose obligations on parties after the termination or expiration of a contract. This includes various commitments such as post-termination transition, payment responsibilities, intellectual property transfers, and other similar obligations. The task specifically measures the legal capability to interpret contractual language and determine whether a clause fits within the defined category of post-termination services.\n\nTo successfully complete this task, legal reasoning and analysis are required to interpret the nuances of contractual language. The model must discern whether the obligations outlined in a clause extend beyond the contract's termination, which is critical for understanding the ongoing responsibilities of the parties involved. This task is significant for evaluating legal AI systems as it tests their proficiency in contract interpretation, a fundamental aspect of legal practice. Accurate classification of post-termination obligations is essential for legal professionals to ensure compliance and mitigate risks associated with contract termination.\n\nThe background of this task is rooted in contract law, where post-termination clauses play a vital role in defining the rights and responsibilities of the parties involved after a contract ends. Understanding these obligations is crucial for effective legal counsel and contract management, making this task a valuable benchmark for assessing the capabilities of AI in legal contexts.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 814,
  "tags": [
    "LegalBench",
    "contract law",
    "interpretation",
    "post-termination obligations",
    "rule application"
  ],
  "document_type": "contract clause",
  "min_input_length": 13,
  "max_input_length": 607,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/cuad>)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}