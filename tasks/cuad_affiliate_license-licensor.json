{
  "task_id": "cuad_affiliate_license-licensor",
  "name": "cuad_affiliate_license-licensor",
  "family": "LegalBench",
  "short_description": "Classifies clauses as affiliate license grants by licensors or not.",
  "long_description": "The 'cuad_affiliate_license-licensor' task evaluates the ability to identify contractual clauses that describe a license grant by affiliates of the licensor or that involve intellectual property owned by these affiliates. This task specifically measures the legal capability to interpret and classify contract language, focusing on the nuances of licensing agreements and the roles of affiliated entities in such agreements. Understanding these distinctions is crucial in contract law, as it can significantly impact the rights and obligations of the parties involved.\n\nTo successfully complete this task, legal reasoning and analysis are required to discern whether a clause meets the criteria of an affiliate license grant. This involves interpreting the language of the clause, understanding the context of affiliate relationships, and applying knowledge of intellectual property rights. The importance of this task lies in its relevance to evaluating legal AI systems, as it tests the system's ability to navigate complex legal language and accurately classify contractual provisions, which is essential for effective legal document review and analysis. The task draws on fundamental concepts in contract law and intellectual property, making it a valuable benchmark for assessing AI performance in legal contexts.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 94,
  "tags": [
    "LegalBench",
    "contract law",
    "intellectual property",
    "interpretation"
  ],
  "document_type": "contract clause",
  "min_input_length": 24,
  "max_input_length": 746,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/cuad>)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}