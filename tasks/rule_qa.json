{
  "task_id": "rule_qa",
  "name": "rule_qa",
  "family": "LegalBench",
  "short_description": "Evaluates knowledge of federal and state legal rules and doctrines.",
  "long_description": "The Rule QA task measures the ability of legal AI systems to accurately recall and articulate specific legal rules and doctrines from both federal and state law. This task requires the model to engage in rule-recall legal reasoning, where it must not only identify legal principles but also provide their formulations and relevant codifications. The questions posed may include inquiries about specific legal concepts, such as hearsay, or broader questions regarding the organization of legal doctrines within various areas of law, such as Civil Procedure or Contract Law.\n\nThis task is crucial for evaluating legal AI systems as it tests their foundational understanding of legal rules, which is essential for any legal analysis or application. By assessing the model's ability to generate correct answers in a zero-shot setting, we can determine its readiness for real-world legal applications where precise legal knowledge is paramount. The importance of this task lies in its potential to enhance the accuracy and reliability of AI systems in legal practice, ensuring that they can effectively support legal professionals in their decision-making processes.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 50,
  "tags": [
    "LegalBench",
    "civil procedure",
    "hearsay",
    "rule application"
  ],
  "document_type": "legal question",
  "min_input_length": 8,
  "max_input_length": 24,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "Neel Guha",
  "license": "[CC by 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Rule-recall",
  "task_type": "Generation"
}