{
  "task_id": "cuad_governing_law",
  "name": "cuad_governing_law",
  "family": "LegalBench",
  "short_description": "Classify if a contract clause specifies the governing law.",
  "long_description": "The 'cuad_governing_law' task evaluates the ability of legal AI systems to identify whether a contractual clause explicitly states which jurisdiction's laws govern the contract. This task measures the AI's capability to interpret legal language and recognize the significance of governing law clauses, which are crucial for determining the applicable legal framework in case of disputes. Understanding governing law is fundamental in contract law as it influences the interpretation and enforcement of contractual obligations across different jurisdictions.\n\nThis task requires the application of legal reasoning and analysis, particularly in interpreting the language of contracts. The model must discern between clauses that specify governing law and those that do not, which involves a nuanced understanding of legal terminology and context. Evaluating AI systems on this task is vital, as accurate identification of governing law clauses can significantly impact legal outcomes and the effectiveness of automated contract review processes. The ability to correctly classify these clauses also reflects the broader competency of AI in navigating complex legal documents and ensuring compliance with jurisdictional requirements.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 882,
  "tags": [
    "LegalBench",
    "contract law",
    "interpretation",
    "jurisdiction"
  ],
  "document_type": "contract clause",
  "min_input_length": 15,
  "max_input_length": 607,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/cuad>)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}