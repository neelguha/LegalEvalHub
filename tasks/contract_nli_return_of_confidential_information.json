{
  "task_id": "contract_nli_return_of_confidential_information",
  "name": "contract_nli_return_of_confidential_information",
  "family": "LegalBench",
  "short_description": "Evaluates if an NDA clause mandates return or destruction of confidential information.",
  "long_description": "The 'contract_nli_return_of_confidential_information' task measures the legal capability to interpret clauses within Non-Disclosure Agreements (NDAs) regarding the handling of confidential information upon termination of the agreement. Specifically, it assesses whether a clause explicitly states that the Receiving Party must either return or destroy confidential information, which is a critical aspect of confidentiality obligations in legal contracts. This task requires nuanced legal reasoning and analysis, as it involves understanding the implications of contractual language and the legal consequences of failing to comply with such obligations.\n\nThis task is vital for evaluating legal AI systems as it tests their ability to accurately interpret and classify legal clauses, which is essential for ensuring compliance with confidentiality requirements in various legal contexts. The ability to discern the specific obligations imposed by NDAs not only reflects a system's understanding of contract law but also its capability to assist legal professionals in drafting, reviewing, and enforcing such agreements. Given the increasing reliance on automated systems in legal practice, ensuring that AI can effectively interpret these critical legal concepts is paramount for maintaining the integrity of legal processes and protecting sensitive information.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 74,
  "tags": [
    "LegalBench",
    "confidentiality",
    "contract law",
    "interpretation"
  ],
  "document_type": "contract clause",
  "min_input_length": 31,
  "max_input_length": 335,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[ContractNLI](https://stanfordnlp.github.io/contract-nli/)",
  "license": "[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}