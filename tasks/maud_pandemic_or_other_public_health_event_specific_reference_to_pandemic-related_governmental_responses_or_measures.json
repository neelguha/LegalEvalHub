{
  "task_id": "maud_pandemic_or_other_public_health_event_specific_reference_to_pandemic-related_governmental_responses_or_measures",
  "name": "maud_pandemic_or_other_public_health_event_specific_reference_to_pandemic-related_governmental_responses_or_measures",
  "family": "LegalBench",
  "short_description": "Evaluates references to pandemic-related measures in merger agreements.",
  "long_description": "This task measures the legal capability to interpret merger agreements, specifically focusing on identifying whether there are references to governmental responses or measures related to pandemics or other public health events that qualify as Material Adverse Effects (MAE). It requires an understanding of how such references can impact the contractual obligations and risk assessments of the parties involved in a merger. Legal professionals must be adept at analyzing contract language to determine the implications of these references on the transaction's viability and the parties' liabilities.\n\nThe task necessitates a nuanced legal reasoning process, as it involves binary classification based on the interpretation of specific clauses within merger agreements. Evaluating the presence or absence of pandemic-related references is crucial, especially in the context of the ongoing legal and economic ramifications of public health events. This task is important for evaluating legal AI systems because it tests their ability to understand complex legal language and context, which is essential for providing accurate legal insights and supporting decision-making in corporate transactions. Understanding these legal concepts is vital for practitioners, as they navigate the evolving landscape of merger agreements in light of unprecedented public health challenges.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 99,
  "tags": [
    "LegalBench",
    "corporate law",
    "interpretation",
    "material adverse effect"
  ],
  "document_type": "merger agreement",
  "min_input_length": 623,
  "max_input_length": 1744,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}