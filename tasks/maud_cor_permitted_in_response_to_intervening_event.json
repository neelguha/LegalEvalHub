{
  "task_id": "maud_cor_permitted_in_response_to_intervening_event",
  "name": "maud_cor_permitted_in_response_to_intervening_event",
  "family": "LegalBench",
  "short_description": "Evaluates if a Change of Recommendation is allowed after an intervening event in mergers.",
  "long_description": "The task 'maud_cor_permitted_in_response_to_intervening_event' measures the legal capability to interpret merger agreements, specifically focusing on whether a Change of Recommendation is permissible in response to an intervening event. This involves analyzing specific clauses within a merger agreement to determine the conditions under which such a change may occur, reflecting an understanding of contractual obligations and the nuances of corporate law. The task requires the application of legal reasoning to assess the implications of the language used in the agreement and the context of the intervening event.\n\nThis task is crucial for evaluating legal AI systems as it tests their ability to comprehend complex legal documents and apply legal principles accurately. Understanding the permissibility of actions like a Change of Recommendation is vital for parties involved in mergers and acquisitions, as it can significantly impact negotiations and strategic decisions. The background legal concepts include the interpretation of contractual terms and the implications of intervening events, which are common in corporate transactions and can affect the obligations and rights of the parties involved.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 101,
  "tags": [
    "LegalBench",
    "corporate law",
    "interpretation",
    "merger agreements"
  ],
  "document_type": "merger agreement",
  "min_input_length": 80,
  "max_input_length": 1630,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}