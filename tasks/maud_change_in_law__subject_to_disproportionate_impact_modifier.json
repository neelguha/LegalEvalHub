{
  "task_id": "maud_change_in_law__subject_to_disproportionate_impact_modifier",
  "name": "maud_change_in_law__subject_to_disproportionate_impact_modifier",
  "family": "LegalBench",
  "short_description": "Evaluates if disproportionate legal changes qualify as Material Adverse Effect in mergers.",
  "long_description": "This task measures the legal capability to interpret merger agreements, specifically focusing on the implications of changes in law that may have a disproportionate impact on the parties involved. It requires an understanding of the concept of Material Adverse Effect (MAE) within the context of merger agreements, as well as the ability to analyze how specific legal changes could affect the overall transaction. The task challenges the evaluator to apply legal reasoning to determine whether such changes qualify under the defined terms of the agreement, showcasing the nuanced understanding required in corporate law contexts.\n\nThe importance of this task lies in its relevance to evaluating legal AI systems, particularly in their ability to navigate complex legal language and concepts. As mergers and acquisitions are critical to business operations, understanding the implications of legal changes is essential for risk assessment and decision-making. This task not only tests the AI's interpretative skills but also its capacity to apply legal knowledge in a practical scenario, reflecting the real-world challenges faced by legal professionals when drafting and negotiating merger agreements.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 100,
  "tags": [
    "LegalBench",
    "Material Adverse Effect",
    "corporate law",
    "interpretation"
  ],
  "document_type": "merger agreement",
  "min_input_length": 623,
  "max_input_length": 1744,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}