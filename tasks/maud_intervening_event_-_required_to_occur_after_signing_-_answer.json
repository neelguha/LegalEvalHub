{
  "task_id": "maud_intervening_event_-_required_to_occur_after_signing_-_answer",
  "name": "maud_intervening_event_-_required_to_occur_after_signing_-_answer",
  "family": "LegalBench",
  "short_description": "Evaluates if an 'Intervening Event' is required post-signing in merger agreements.",
  "long_description": "This task measures the legal capability to interpret specific clauses within merger agreements, particularly focusing on the concept of 'Intervening Events.' It assesses the understanding of contractual obligations and the timing of events that may affect the terms of a merger. Participants must analyze a provided excerpt from a merger agreement to determine whether such an event is mandated to occur after the signing of the agreement, which requires a nuanced understanding of contract law and the implications of timing in legal agreements.\n\nLegal reasoning in this task involves careful interpretation of contractual language and the ability to apply legal principles to specific scenarios. This task is crucial for evaluating legal AI systems as it tests their ability to comprehend and analyze complex legal documents, ensuring they can assist legal professionals in making informed decisions based on contractual stipulations. Understanding 'Intervening Events' is vital in merger agreements, as these events can significantly impact the rights and obligations of the parties involved, influencing the overall success of the transaction.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 148,
  "tags": [
    "LegalBench",
    "contract interpretation",
    "interpretation",
    "merger law",
    "rule application"
  ],
  "document_type": "contract clause",
  "min_input_length": 50,
  "max_input_length": 1128,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}