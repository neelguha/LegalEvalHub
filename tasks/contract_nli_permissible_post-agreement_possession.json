{
  "task_id": "contract_nli_permissible_post-agreement_possession",
  "name": "contract_nli_permissible_post-agreement_possession",
  "family": "LegalBench",
  "short_description": "Evaluates if an NDA clause allows retention of Confidential Information post-agreement.",
  "long_description": "The task 'contract_nli_permissible_post-agreement_possession' measures the legal capability to interpret clauses within Non-Disclosure Agreements (NDAs), specifically focusing on whether such clauses permit the Receiving Party to retain Confidential Information after the return or destruction of that information. This evaluation requires an understanding of contract law principles, particularly those related to confidentiality and the obligations of parties under an NDA. Legal professionals must discern the nuances of language used in these agreements to determine the implications of retention rights, which can significantly affect the handling of sensitive information post-agreement.\n\nThis task is crucial for assessing the performance of legal AI systems in interpreting contractual language and understanding the legal ramifications of specific clauses. Accurate interpretation is vital for ensuring compliance with confidentiality obligations and protecting proprietary information. The background legal concepts involved include the principles of confidentiality, the enforceability of contract terms, and the rights and duties of parties in contractual relationships. By evaluating the AI's ability to classify these clauses correctly, stakeholders can gauge the reliability and effectiveness of AI tools in legal practice, particularly in contract analysis and risk assessment.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 119,
  "tags": [
    "LegalBench",
    "confidentiality",
    "contract law",
    "interpretation"
  ],
  "document_type": "contract clause",
  "min_input_length": 20,
  "max_input_length": 418,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[ContractNLI](https://stanfordnlp.github.io/contract-nli/)",
  "license": "[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}