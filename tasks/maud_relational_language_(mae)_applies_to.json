{
  "task_id": "maud_relational_language_(mae)_applies_to",
  "name": "maud_relational_language_(mae)_applies_to",
  "family": "LegalBench",
  "short_description": "Evaluates understanding of MAE carveouts in merger agreements.",
  "long_description": "The 'maud_relational_language_(mae)_applies_to' task measures the ability to interpret and analyze the relational language found in merger agreements, specifically regarding carveouts related to Material Adverse Effect (MAE). This task requires participants to engage in legal reasoning by identifying how specific clauses within the agreement apply to the defined legal concept of MAE. Understanding these carveouts is crucial, as they can significantly impact the obligations and liabilities of the parties involved in a merger, making it essential for legal professionals to accurately interpret these provisions.\n\nThis task is important for evaluating legal AI systems because it tests the AI's capability to comprehend complex legal language and apply it to specific scenarios. By focusing on a well-defined aspect of merger agreements, the task provides a benchmark for assessing the AI's performance in legal interpretation and reasoning. The background concepts involved, such as MAE and its implications in corporate law, are vital for ensuring that legal AI systems can assist in real-world legal analyses and decision-making processes effectively.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 91,
  "tags": [
    "LegalBench",
    "Material Adverse Effect",
    "corporate law",
    "interpretation"
  ],
  "document_type": "merger agreement",
  "min_input_length": 623,
  "max_input_length": 1744,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "3-way classification"
}