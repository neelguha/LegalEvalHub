{
  "task_id": "maud_fiduciary_exception__board_determination_standard",
  "name": "maud_fiduciary_exception__board_determination_standard",
  "family": "LegalBench",
  "short_description": "Evaluates understanding of board actions under merger agreement no-shop provisions.",
  "long_description": "This task measures the legal capability to interpret and analyze the circumstances under which a corporate board may take actions contrary to a no-shop provision in a merger agreement. Specifically, it focuses on the fiduciary duties of the board and the legal implications of those duties when considering alternative acquisition proposals. The task requires the application of legal reasoning to determine when the board's actions would be justified despite the restrictions imposed by the no-shop clause, highlighting the nuances of fiduciary duty in corporate governance.\n\nThe importance of this task lies in its ability to evaluate legal AI systems' understanding of complex corporate law concepts, particularly fiduciary duties and contractual obligations. By analyzing various scenarios where the board may act outside the no-shop provision, the task tests the AI's capacity to discern the legal standards that govern board decision-making in the context of mergers and acquisitions. This understanding is crucial for ensuring that legal AI tools can assist practitioners in navigating the intricate landscape of corporate law, thereby enhancing their effectiveness in real-world applications.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 180,
  "tags": [
    "LegalBench",
    "corporate law",
    "fiduciary duty",
    "interpretation",
    "rule application"
  ],
  "document_type": "merger agreement",
  "min_input_length": 142,
  "max_input_length": 786,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "9-way classification"
}