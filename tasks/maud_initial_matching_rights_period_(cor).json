{
  "task_id": "maud_initial_matching_rights_period_(cor)",
  "name": "maud_initial_matching_rights_period_(cor)",
  "family": "LegalBench",
  "short_description": "Evaluates the identification of initial matching rights period in merger agreements.",
  "long_description": "The 'maud_initial_matching_rights_period_(cor)' task measures the legal capability of interpreting specific provisions within merger agreements, particularly focusing on the duration of the initial matching rights period in the event of a board recommendation change. This task requires a nuanced understanding of contractual language and the ability to extract relevant information from complex legal texts. Participants must analyze excerpts from merger agreements and select the correct duration from multiple-choice options, demonstrating their proficiency in legal interpretation and reasoning.\n\nThis task is crucial for evaluating legal AI systems as it tests their ability to comprehend and analyze intricate legal documents, which is essential for tasks such as due diligence, compliance checks, and contract negotiations. Understanding matching rights is vital in merger and acquisition contexts, as it impacts the negotiation dynamics between parties and can influence the overall success of the transaction. The task is grounded in the principles of corporate law, particularly those governing mergers and acquisitions, making it a valuable benchmark for assessing AI's capabilities in legal reasoning and document analysis.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 159,
  "tags": [
    "LegalBench",
    "corporate law",
    "interpretation",
    "mergers and acquisitions"
  ],
  "document_type": "merger agreement",
  "min_input_length": 90,
  "max_input_length": 1630,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "7-way classification"
}