{
  "task_id": "maud_cor_standard_(intervening_event)",
  "name": "maud_cor_standard_(intervening_event)",
  "family": "LegalBench",
  "short_description": "Evaluates the board's standard for changing merger recommendations post-intervening events.",
  "long_description": "The 'maud_cor_standard_(intervening_event)' task measures the legal capability to interpret merger agreements, specifically focusing on the standards boards must follow when considering changes to their recommendations in light of intervening events. This task requires an understanding of fiduciary duties and the nuances of legal language within merger agreements, as it involves selecting the most appropriate standard from multiple options based on a given excerpt from a merger agreement. Legal professionals must be adept at recognizing how these standards apply in practice, particularly in high-stakes corporate environments.\n\nThis task is crucial for evaluating legal AI systems because it tests their ability to comprehend complex legal texts and apply legal reasoning to real-world scenarios. Understanding the implications of intervening events on board recommendations is vital for compliance and risk management in corporate governance. The task is grounded in the principles of fiduciary duty, which require boards to act in the best interests of shareholders, making it essential for legal AI to accurately interpret and apply these standards in various contexts.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 85,
  "tags": [
    "LegalBench",
    "corporate law",
    "fiduciary duty",
    "interpretation"
  ],
  "document_type": "merger agreement",
  "min_input_length": 80,
  "max_input_length": 1630,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "9-way classification"
}