{
  "task_id": "textualism_tool_dictionaries",
  "name": "textualism_tool_dictionaries",
  "family": "LegalBench",
  "short_description": "Evaluates if judicial opinions apply textualism using dictionary definitions.",
  "long_description": "The 'textualism_tool_dictionaries' task measures the ability of legal AI systems to identify and classify instances where judicial opinions explicitly rely on dictionary definitions for statutory interpretation. This task specifically assesses the AI's understanding of textualism, a legal philosophy that emphasizes the importance of the ordinary meaning of words as defined in dictionaries. By analyzing paragraphs from judicial opinions, the task evaluates whether the text refers to a dictionary as a guiding tool in the interpretation process, thereby highlighting the AI's capability to discern nuanced legal reasoning based on textualist principles.\n\nIn this binary classification task, the AI must determine if a given paragraph not only references a dictionary but also demonstrates its application in the context of the legal interpretation of statutes. This requires a sophisticated level of rhetorical analysis, as the AI must evaluate both explicit mentions of dictionary reliance and implicit applications of dictionary definitions to the facts presented. The importance of this task lies in its ability to enhance the evaluation of legal AI systems, ensuring they can accurately interpret and apply legal texts in a manner consistent with established legal doctrines. Understanding the role of textualism and dictionary definitions is crucial for legal practitioners, as it shapes how laws are interpreted and applied in real-world scenarios.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 111,
  "tags": [
    "LegalBench",
    "legal reasoning",
    "rhetorical understanding",
    "statutory interpretation",
    "textualism"
  ],
  "document_type": "judicial opinion",
  "min_input_length": 25,
  "max_input_length": 888,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "Austin Peters",
  "license": "[CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/)",
  "legal_reasoning_type": "Rhetorical-analysis",
  "task_type": "Binary classification"
}