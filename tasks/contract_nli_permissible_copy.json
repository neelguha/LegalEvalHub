{
  "task_id": "contract_nli_permissible_copy",
  "name": "contract_nli_permissible_copy",
  "family": "LegalBench",
  "short_description": "Evaluates if an NDA clause allows the Receiving Party to copy Confidential Information.",
  "long_description": "The 'contract_nli_permissible_copy' task specifically measures the ability to interpret clauses within Non-Disclosure Agreements (NDAs) regarding the permissibility of copying Confidential Information by the Receiving Party. This task requires legal reasoning and analysis to determine whether the language of the clause explicitly allows for such actions under certain circumstances, which is crucial for understanding the legal implications of confidentiality obligations in contractual relationships.\n\nLegal reasoning in this context involves careful interpretation of contract language, where nuances in wording can significantly affect the rights and responsibilities of the parties involved. By assessing whether a clause permits copying, this task highlights the importance of precise language in contracts and the potential consequences of ambiguous terms. Evaluating this capability is vital for legal AI systems, as it reflects their ability to navigate complex legal texts and provide accurate interpretations that can influence legal outcomes.\n\nUnderstanding the legal concepts surrounding NDAs is essential, as these agreements are foundational in protecting sensitive information in various business contexts. The task draws from a dataset that categorizes clauses based on their legal effects, emphasizing the significance of clarity and specificity in legal drafting. This evaluation not only aids in the development of more sophisticated legal AI tools but also enhances the reliability of automated systems in legal practice, ensuring they can effectively support legal professionals in their work.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 95,
  "tags": [
    "LegalBench",
    "confidentiality",
    "contract law",
    "interpretation"
  ],
  "document_type": "contract clause",
  "min_input_length": 19,
  "max_input_length": 418,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[ContractNLI](https://stanfordnlp.github.io/contract-nli/)",
  "license": "[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}