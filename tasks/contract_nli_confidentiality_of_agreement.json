{
  "task_id": "contract_nli_confidentiality_of_agreement",
  "name": "contract_nli_confidentiality_of_agreement",
  "family": "LegalBench",
  "short_description": "Evaluates if a clause prohibits disclosing the existence of an agreement.",
  "long_description": "The 'contract_nli_confidentiality_of_agreement' task measures the legal capability to interpret confidentiality clauses within Non-Disclosure Agreements (NDAs). Specifically, it assesses whether a given clause stipulates that the Receiving Party is prohibited from disclosing the fact that the agreement was negotiated or agreed upon. This task requires nuanced legal reasoning and analysis, as it involves understanding the implications of contractual language and the legal effects of confidentiality obligations.\n\nThis task is crucial for evaluating legal AI systems because it tests their ability to comprehend and analyze specific legal provisions that are fundamental to maintaining confidentiality in business transactions. The ability to accurately interpret such clauses is essential for legal practitioners and businesses alike, as breaches of confidentiality can lead to significant legal repercussions. Understanding the subtleties of contract language is vital for ensuring compliance with legal standards and protecting sensitive information, making this task a valuable benchmark for AI systems designed to assist in legal contexts.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 90,
  "tags": [
    "LegalBench",
    "confidentiality",
    "contract law",
    "interpretation"
  ],
  "document_type": "contract clause",
  "min_input_length": 24,
  "max_input_length": 505,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[ContractNLI](https://stanfordnlp.github.io/contract-nli/)",
  "license": "[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}