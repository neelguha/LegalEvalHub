{
  "task_id": "maud_ordinary_course_efforts_standard",
  "name": "maud_ordinary_course_efforts_standard",
  "family": "LegalBench",
  "short_description": "Evaluates understanding of efforts standards in merger agreements.",
  "long_description": "The 'maud_ordinary_course_efforts_standard' task measures the legal capability to interpret and classify the efforts standards outlined in merger agreements. Specifically, it assesses the ability to identify and differentiate between various types of efforts standards, such as 'commercially reasonable efforts', 'flat covenant', and 'reasonable best efforts'. This task requires legal reasoning that involves careful analysis of contractual language and an understanding of the implications of different efforts standards on the obligations of the parties involved in a merger.\n\nThis task is crucial for evaluating legal AI systems as it reflects their ability to comprehend complex legal texts and apply legal knowledge in a practical context. The ability to interpret merger agreements accurately is essential for legal practitioners, as these documents often contain nuanced language that can significantly impact the rights and responsibilities of the parties. Understanding the efforts standard is particularly important in merger agreements, as it dictates the level of diligence and action required from the parties in fulfilling their contractual obligations, thereby influencing the overall success of the transaction.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 182,
  "tags": [
    "LegalBench",
    "corporate law",
    "interpretation",
    "merger agreements"
  ],
  "document_type": "merger agreement",
  "min_input_length": 33,
  "max_input_length": 591,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "3-way classification"
}