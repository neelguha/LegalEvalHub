{
  "task_id": "proa",
  "name": "proa",
  "family": "LegalBench",
  "short_description": "Evaluate if a statute grants a private right of action for individuals.",
  "long_description": "The 'proa' task measures the ability to identify whether a statute explicitly provides a private right of action (PROA), which allows individuals to enforce their rights through legal action. This capability is crucial for understanding the enforcement mechanisms available to private citizens under various laws, particularly in areas such as antitrust and environmental law, where individuals may seek redress for harm caused by unlawful conduct. The task requires a nuanced interpretation of statutory language to determine the presence or absence of such rights, which can significantly impact legal outcomes for affected individuals.\n\nLegal reasoning in this task involves careful analysis of statutory text to discern the legislative intent behind the provision. By classifying clauses as containing or not containing a PROA, this task aids in assessing the accessibility of legal remedies for individuals, thereby influencing the broader implications for justice and accountability in society. Evaluating AI systems on this task is essential, as it tests their proficiency in legal interpretation and their ability to navigate complex legal language, which is fundamental for effective legal practice and ensuring that individuals can assert their rights in court.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 100,
  "tags": [
    "LegalBench",
    "interpretation",
    "private right of action",
    "statute"
  ],
  "document_type": "statute",
  "min_input_length": 20,
  "max_input_length": 153,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "Neel Guha, Diego Zambrano",
  "license": "[CC by 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}