{
  "task_id": "maud_fls_(mae)_standard",
  "name": "maud_fls_(mae)_standard",
  "family": "LegalBench",
  "short_description": "Evaluates understanding of Forward Looking Standard in merger agreements.",
  "long_description": "The 'maud_fls_(mae)_standard' task measures the ability to interpret and analyze specific provisions related to the Forward Looking Standard (FLS) in the context of Material Adverse Effect (MAE) within merger agreements. This task requires the participant to read an excerpt from a merger agreement and accurately identify how the FLS is characterized, which is crucial for understanding the implications of MAE clauses in corporate transactions. The task involves interpreting legal language and selecting the most appropriate answer from multiple choices, reflecting the nuances of contractual obligations and expectations in mergers and acquisitions.\n\nThis task is significant for evaluating legal AI systems as it tests their capability to comprehend complex legal texts and apply legal reasoning to real-world scenarios. Understanding the FLS in relation to MAE is vital for legal practitioners, as it influences risk assessments and decision-making processes in mergers. The task is constructed from a rich dataset of merger agreements, ensuring that the evaluation is grounded in practical legal contexts and reflects the standards used by professionals in the field.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 78,
  "tags": [
    "LegalBench",
    "Material Adverse Effect",
    "corporate law",
    "interpretation"
  ],
  "document_type": "merger agreement",
  "min_input_length": 623,
  "max_input_length": 1559,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}