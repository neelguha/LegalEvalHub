{
  "task_id": "maud_general_economic_and_financial_conditions_subject_to_disproportionate_impact_modifier",
  "name": "maud_general_economic_and_financial_conditions_subject_to_disproportionate_impact_modifier",
  "family": "LegalBench",
  "short_description": "Evaluates the interpretation of economic impacts on Material Adverse Effect in mergers.",
  "long_description": "This task measures the legal capability to interpret merger agreements, specifically focusing on how general economic and financial conditions that disproportionately impact a party may qualify as a Material Adverse Effect (MAE). It requires the legal reasoning skill of interpretation, as participants must analyze specific excerpts from merger agreements and apply their understanding of legal definitions and implications regarding MAE. The task is designed to assess the ability of legal AI systems to navigate complex legal language and concepts within the context of corporate law.\n\nUnderstanding the nuances of MAE is crucial in merger agreements, as it can significantly affect the parties' obligations and rights during the transaction. The task is important for evaluating legal AI systems because it tests their ability to discern subtle distinctions in legal language and apply relevant legal standards to specific scenarios. This capability is essential for ensuring that AI systems can assist legal professionals in making informed decisions based on accurate interpretations of contractual terms and conditions.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 99,
  "tags": [
    "LegalBench",
    "Material Adverse Effect",
    "corporate law",
    "interpretation"
  ],
  "document_type": "merger agreement",
  "min_input_length": 623,
  "max_input_length": 1744,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}