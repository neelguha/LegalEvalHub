{
  "task_id": "cuad_affiliate_license-licensee",
  "name": "cuad_affiliate_license-licensee",
  "family": "LegalBench",
  "short_description": "Classifies clauses that describe license grants to licensees and their affiliates.",
  "long_description": "The 'cuad_affiliate_license-licensee' task evaluates the ability to identify contractual clauses that specifically grant licenses to licensees, including sublicensors and their affiliates. This task measures the legal capability of interpreting contractual language and understanding the implications of licensing agreements within the context of affiliate relationships. It is crucial for legal AI systems to accurately discern these clauses to ensure compliance and proper application of licensing terms in various legal scenarios.\n\nThis task requires nuanced legal reasoning and analysis, as it involves distinguishing between clauses that explicitly grant licenses and those that do not. The importance of this task lies in its relevance to contract law, particularly in the context of intellectual property and business transactions where licensing agreements are common. Understanding the scope of a license grant is essential for assessing rights and obligations under the contract, making this task a vital benchmark for evaluating the performance of legal AI systems in real-world applications.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 204,
  "tags": [
    "LegalBench",
    "contract law",
    "interpretation",
    "license grant"
  ],
  "document_type": "contract clause",
  "min_input_length": 13,
  "max_input_length": 746,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/cuad>)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}