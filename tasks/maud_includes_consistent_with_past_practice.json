{
  "task_id": "maud_includes_consistent_with_past_practice",
  "name": "maud_includes_consistent_with_past_practice",
  "family": "LegalBench",
  "short_description": "Evaluates if a merger agreement's Efforts Covenant includes 'consistent with past practice'.",
  "long_description": "The task 'maud_includes_consistent_with_past_practice' measures the legal capability to interpret specific clauses within merger agreements, particularly focusing on the Efforts Covenant. This task requires the ability to analyze legal language and determine whether it aligns with established practices in past agreements. The legal reasoning involved is primarily interpretative, necessitating a nuanced understanding of contractual language and its implications in the context of merger agreements.\n\nThis task is crucial for evaluating legal AI systems as it tests their capacity to accurately discern and classify contractual language, which is fundamental in legal practice. Understanding phrases like 'consistent with past practice' is essential for legal professionals, as such language can significantly influence the obligations and expectations of parties involved in a merger. The task draws on the MAUD dataset, which provides a rich source of annotated merger agreements, allowing for a thorough assessment of AI models in understanding complex legal documents.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 182,
  "tags": [
    "LegalBench",
    "corporate law",
    "interpretation",
    "merger agreement"
  ],
  "document_type": "merger agreement",
  "min_input_length": 33,
  "max_input_length": 591,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}