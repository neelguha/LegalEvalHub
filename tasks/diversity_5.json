{
  "task_id": "diversity_5",
  "name": "diversity_5",
  "family": "LegalBench",
  "short_description": "Evaluates understanding of diversity jurisdiction in federal court claims.",
  "long_description": "The 'diversity_5' task measures the legal capability to assess whether the criteria for diversity jurisdiction are satisfied in federal court cases. Specifically, it evaluates the ability to determine complete diversity between plaintiffs and defendants, as well as the adequacy of the amount-in-controversy (AiC) threshold of over $75,000. This task requires a nuanced understanding of how citizenships of the parties involved interact with jurisdictional rules, particularly in scenarios involving multiple plaintiffs and claims against a single defendant.\n\nLegal reasoning in this task involves rule application and conclusion drawing based on the established legal standards for diversity jurisdiction. Participants must analyze fact patterns that present various combinations of citizenships and claims, applying the rules of aggregation and diversity to arrive at a binary classification of whether diversity jurisdiction exists. This task is crucial for evaluating legal AI systems as it tests their ability to navigate complex jurisdictional issues that are foundational to federal court procedures, ensuring that AI can assist in identifying appropriate venues for legal disputes.\n\nDiversity jurisdiction is a fundamental concept in U.S. law, designed to prevent bias in state courts by allowing federal courts to hear cases involving parties from different states. Understanding this concept is essential for legal practitioners and AI systems alike, as it directly impacts the strategy and venue selection in litigation. The task's design, which includes synthetic fact patterns, allows for a robust evaluation of AI's capabilities in handling real-world legal scenarios.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 306,
  "tags": [
    "LegalBench",
    "diversity jurisdiction",
    "jurisdiction",
    "rule application",
    "rule conclusion"
  ],
  "document_type": "fact pattern",
  "min_input_length": 46,
  "max_input_length": 64,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "Neel Guha",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Rule-application/Rule-conclusion",
  "task_type": "Binary classification"
}