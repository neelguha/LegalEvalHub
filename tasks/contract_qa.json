{
  "task_id": "contract_qa",
  "name": "contract_qa",
  "family": "LegalBench",
  "short_description": "Evaluates AI's ability to identify specific issues in contractual clauses.",
  "long_description": "The 'contract_qa' task measures the capability of legal AI systems to interpret and analyze contractual language by determining whether specific clauses address various legal issues. This task requires the AI to engage in nuanced legal reasoning and interpretation, as it must assess excerpts from contracts and answer binary yes/no questions regarding the presence of particular content, such as confidentiality requirements, arbitration clauses, and compliance with privacy laws. The challenge lies in the AI's ability to identify clause types that may not have been explicitly demonstrated in the training examples, thereby testing its generalization and understanding of legal language.\n\nThis task is crucial for evaluating the effectiveness of legal AI systems, as it reflects their ability to navigate complex legal documents and extract relevant information accurately. Understanding the presence of specific clauses is essential for legal practitioners who rely on contracts to define rights and obligations. The task encompasses a variety of legal concepts, including consumer privacy laws, dispute resolution mechanisms, and compliance with regulations, making it a comprehensive benchmark for assessing the interpretative skills of AI in the legal domain.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 88,
  "tags": [
    "LegalBench",
    "contract law",
    "interpretation",
    "legal knowledge"
  ],
  "document_type": "contract clause",
  "min_input_length": 23,
  "max_input_length": 141,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "Nikon Rasumov-Rahe, Aditya Narayana, and Dmitry Talisman",
  "license": "[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}