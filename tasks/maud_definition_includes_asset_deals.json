{
  "task_id": "maud_definition_includes_asset_deals",
  "name": "maud_definition_includes_asset_deals",
  "family": "LegalBench",
  "short_description": "Evaluates understanding of superior offers in asset deals within merger agreements.",
  "long_description": "The task 'maud_definition_includes_asset_deals' measures the legal capability to interpret and analyze specific provisions within merger agreements, particularly in relation to what constitutes a 'superior offer' in asset deals. This involves understanding the nuances of contractual language and the implications of various thresholds for asset acquisition, which are critical in the context of mergers and acquisitions. The task requires the ability to discern between different classifications of offers based on the language used in the agreement, showcasing a deep understanding of corporate law principles and contractual obligations.\n\nLegal reasoning in this task focuses on interpretation, as participants must evaluate excerpts from merger agreements and apply their knowledge of deal structures to select the most accurate characterization of a superior offer. This task is essential for evaluating legal AI systems because it tests their ability to navigate complex legal texts, which is a fundamental skill for legal practitioners. The background concepts involved, such as asset deals and the criteria for superior offers, are pivotal in corporate transactions, making this task relevant for ensuring that AI systems can support legal professionals in real-world scenarios.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 147,
  "tags": [
    "LegalBench",
    "corporate law",
    "interpretation",
    "merger agreement"
  ],
  "document_type": "merger agreement",
  "min_input_length": 90,
  "max_input_length": 1606,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "4-way classification"
}