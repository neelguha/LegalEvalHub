{
  "task_id": "canada_tax_court_outcomes",
  "name": "canada_tax_court_outcomes",
  "family": "LegalBench",
  "short_description": "Classify outcomes of appeals in Canada Tax Court decisions as allowed or dismissed.",
  "long_description": "The 'canada_tax_court_outcomes' task evaluates the ability to identify and classify the outcomes of appeals in excerpts from decisions made by the Tax Court of Canada. Specifically, it measures the capability to discern whether an excerpt includes a clear outcome of an appeal and, if so, to categorize that outcome as either 'allowed', 'dismissed', or 'other'. This task requires a nuanced understanding of legal reasoning, particularly rhetorical analysis, as it involves interpreting legal language and determining the implications of the court's wording regarding the appeal's success or failure.\n\nThis task is crucial for assessing legal AI systems because it reflects their ability to process and analyze judicial opinions, which is a fundamental aspect of legal research and practice. By accurately classifying case outcomes, legal AI can contribute to empirical legal research, helping scholars and practitioners understand trends in tax law and the effectiveness of legal arguments in appeals. The task is grounded in the legal principles surrounding tax appeals in Canada, where outcomes can significantly affect taxpayers and the interpretation of tax law, making it essential for legal professionals to have reliable tools for outcome analysis.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 250,
  "tags": [
    "LegalBench",
    "judicial opinion",
    "rhetorical understanding",
    "rule application",
    "tax law"
  ],
  "document_type": "judicial opinion",
  "min_input_length": 64,
  "max_input_length": 1018,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "Sean Rehaag",
  "license": "[CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/)",
  "legal_reasoning_type": "Rhetorical-analysis",
  "task_type": "3-way classification"
}