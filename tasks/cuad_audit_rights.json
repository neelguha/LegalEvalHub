{
  "task_id": "cuad_audit_rights",
  "name": "cuad_audit_rights",
  "family": "LegalBench",
  "short_description": "Classifies if a clause grants audit rights for compliance verification.",
  "long_description": "The 'cuad_audit_rights' task evaluates the ability to identify whether a contractual clause provides a party the right to audit the books, records, or physical locations of the counterparty. This task measures the legal capability to interpret contract language specifically related to audit rights, which are crucial for ensuring compliance with contractual obligations. The task involves binary classification, requiring the model to discern between clauses that explicitly grant such rights and those that do not, thereby assessing the model's understanding of legal terminology and implications in contractual contexts.\n\nThis task is significant for evaluating legal AI systems as it tests their proficiency in interpreting complex legal documents, which is essential for tasks such as contract review and compliance monitoring. Understanding audit rights is vital in various legal scenarios, including corporate governance and risk management, where parties need assurance that contractual terms are being adhered to. The 'cuad_audit_rights' task is constructed from the CUAD dataset, which consists of expert-annotated clauses, ensuring that the evaluation is grounded in real-world legal practices and standards.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 1222,
  "tags": [
    "LegalBench",
    "audit rights",
    "contract law",
    "interpretation"
  ],
  "document_type": "contract clause",
  "min_input_length": 11,
  "max_input_length": 721,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/cuad>)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}