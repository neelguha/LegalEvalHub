{
  "task_id": "maud_pandemic_or_other_public_health_event__subject_to_disproportionate_impact_modifier",
  "name": "maud_pandemic_or_other_public_health_event__subject_to_\"disproportionate_impact\"_modifier",
  "family": "LegalBench",
  "short_description": "Evaluates interpretation of MAE clauses related to public health events in mergers.",
  "long_description": "This task measures the legal capability to interpret merger agreement clauses, specifically focusing on the conditions under which pandemics or other public health events may qualify as a Material Adverse Effect (MAE). It requires an understanding of how legal language defines and constrains the applicability of MAE in the context of unforeseen events, emphasizing the necessity of establishing a 'disproportionate impact' to invoke such clauses. The task involves analyzing specific excerpts from merger agreements to determine the legal implications of these terms.\n\nThe reasoning required for this task is primarily interpretative, as it necessitates the ability to discern the nuances in legal language and apply them to hypothetical scenarios. This evaluation is crucial for assessing legal AI systems, as it tests their proficiency in understanding complex legal documents and their ability to provide accurate interpretations that align with established legal principles. Given the increasing prevalence of public health crises, understanding how these events interact with contractual obligations is vital for legal practitioners and businesses alike, making this task particularly relevant in today's legal landscape.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 99,
  "tags": [
    "LegalBench",
    "Material Adverse Effect",
    "corporate law",
    "interpretation"
  ],
  "document_type": "contract clause",
  "min_input_length": 623,
  "max_input_length": 1744,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}