{
  "task_id": "maud_accuracy_of_fundamental_target_rws_bringdown_standard",
  "name": "maud_accuracy_of_fundamental_target_rws_bringdown_standard",
  "family": "LegalBench",
  "short_description": "Evaluates accuracy requirements for representations in merger agreements.",
  "long_description": "The task 'maud_accuracy_of_fundamental_target_rws_bringdown_standard' measures the legal capability to interpret the accuracy standards for fundamental representations and warranties as stipulated in merger agreements. Specifically, it assesses the understanding of how these standards are articulated in the bring down provision of such agreements, which is crucial for determining the obligations of parties involved in a merger. This task requires the ability to analyze legal language and discern the nuances of materiality standards, which can significantly impact the rights and liabilities of the parties post-closing.\n\nLegal reasoning in this task involves interpretation of contractual language and the application of legal principles related to merger agreements. Participants must evaluate excerpts from merger agreements and select the most accurate characterization of the accuracy requirements based on the options provided. This task is vital for evaluating legal AI systems as it tests their ability to navigate complex legal texts and apply relevant legal standards, ensuring that AI can assist legal professionals in accurately interpreting and negotiating merger agreements. Understanding these concepts is essential for practitioners in corporate law, as they directly influence deal structures and risk assessments in mergers and acquisitions.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 176,
  "tags": [
    "LegalBench",
    "corporate law",
    "interpretation",
    "merger agreement",
    "rule application"
  ],
  "document_type": "merger agreement",
  "min_input_length": 61,
  "max_input_length": 675,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "3-way classification"
}