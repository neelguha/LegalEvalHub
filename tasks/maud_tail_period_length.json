{
  "task_id": "maud_tail_period_length",
  "name": "maud_tail_period_length",
  "family": "LegalBench",
  "short_description": "Evaluates the identification of Tail Period length in merger agreements.",
  "long_description": "The 'maud_tail_period_length' task measures the ability to interpret and analyze specific provisions within merger agreements, particularly focusing on the Tail Period, which is a critical component in the context of mergers and acquisitions. This task requires legal reasoning skills to accurately identify the duration specified in the agreement, which can vary significantly depending on the terms negotiated by the parties involved. Understanding the Tail Period is essential, as it can impact post-merger obligations and the timing of various rights and responsibilities that arise after the merger is completed.\n\nThis task is important for evaluating legal AI systems because it tests the model's capability to comprehend and apply legal concepts within a practical context. The interpretation of contractual terms like the Tail Period is fundamental in corporate law, where precise language can have substantial implications for the parties involved. By analyzing excerpts from actual merger agreements, the task not only assesses the AI's understanding of legal language but also its ability to navigate the complexities of corporate transactions, making it a valuable benchmark for legal AI performance.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 180,
  "tags": [
    "LegalBench",
    "corporate law",
    "interpretation",
    "merger agreements"
  ],
  "document_type": "merger agreement",
  "min_input_length": 49,
  "max_input_length": 875,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "5-way classification"
}