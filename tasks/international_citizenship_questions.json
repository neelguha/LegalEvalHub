{
  "task_id": "international_citizenship_questions",
  "name": "international_citizenship_questions",
  "family": "LegalBench",
  "short_description": "Evaluates understanding of global citizenship law through Yes/No questions.",
  "long_description": "The 'international_citizenship_questions' task measures the legal capability to recall and apply citizenship laws from various jurisdictions around the world. It focuses on the ability to accurately interpret statutory provisions related to citizenship, requiring participants to answer binary questions based on the GLOBALCIT Citizenship Law Dataset. This task emphasizes rule-recall legal reasoning, where the correct application of legal rules is essential for determining citizenship eligibility and related rights in different countries.\n\nThis task is crucial for evaluating legal AI systems as it tests their proficiency in navigating complex legal frameworks that vary significantly across jurisdictions. Understanding citizenship law is vital not only for legal practitioners but also for individuals seeking to understand their rights and obligations in a global context. The task highlights the importance of precise legal knowledge and the ability to synthesize information from diverse legal sources, making it a valuable benchmark for assessing the capabilities of AI in legal reasoning and statutory interpretation.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 9310,
  "tags": [
    "LegalBench",
    "citizenship law",
    "rule application",
    "statutory interpretation"
  ],
  "document_type": "legal question",
  "min_input_length": 20,
  "max_input_length": 75,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[GLOBALCIT Citizenship Law Dataset](https://globalcit.eu/modes-acquisition-citizenship/)",
  "license": "[CC by 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Rule-recall",
  "task_type": "Binary classification"
}