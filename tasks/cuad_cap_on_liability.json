{
  "task_id": "cuad_cap_on_liability",
  "name": "cuad_cap_on_liability",
  "family": "LegalBench",
  "short_description": "Classifies clauses specifying caps on liability in contracts.",
  "long_description": "The 'cuad_cap_on_liability' task evaluates the ability to identify contractual clauses that specify limitations on liability, particularly in the context of breaches of obligations. This includes determining whether a clause sets a maximum amount for recovery or imposes a time limitation for a counterparty to bring claims. By focusing on these specific elements, the task measures the legal capability to interpret and analyze contractual language related to liability caps, which is crucial for understanding the extent of legal obligations and protections in contractual agreements.\n\nLegal reasoning in this task requires careful interpretation of contract clauses, as the implications of liability caps can significantly affect the rights and responsibilities of the parties involved. Misinterpretation can lead to substantial legal and financial consequences. Evaluating AI systems on this task is essential because it tests their proficiency in understanding complex legal language and their ability to apply legal principles accurately. Given the prevalence of liability caps in various contracts, this task is vital for ensuring that AI tools can assist legal professionals in contract review and risk assessment effectively.\n\nThe task is grounded in fundamental legal concepts related to contract law, particularly the principles governing liability and the enforceability of contractual terms. Understanding these concepts is essential for legal practitioners who must navigate the intricacies of liability clauses in their work, making this task a valuable benchmark for assessing the capabilities of legal AI systems.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 1252,
  "tags": [
    "LegalBench",
    "contract law",
    "interpretation",
    "liability"
  ],
  "document_type": "contract clause",
  "min_input_length": 11,
  "max_input_length": 543,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/cuad>)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}