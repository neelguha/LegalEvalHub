{
  "task_id": "overruling",
  "name": "overruling",
  "family": "LegalBench",
  "short_description": "Classify if a judicial opinion sentence overrules a prior case.",
  "long_description": "The 'overruling' task evaluates the capability to determine whether a specific sentence from a judicial opinion effectively overturns the decision of a previous case. This task measures legal reasoning and analysis skills, particularly in the context of understanding the implications of judicial language and the hierarchy of case law. It requires a nuanced understanding of legal precedent and the ability to interpret the rhetorical structure of judicial opinions, which is critical for legal practitioners who must navigate complex legal landscapes and apply relevant case law accurately.\n\nThis task is essential for evaluating legal AI systems as it tests their ability to comprehend and analyze legal texts in a manner similar to human legal professionals. The ability to identify when a court decision overrules a prior ruling is fundamental to legal practice, as it directly impacts the application of law and the predictability of legal outcomes. Understanding the concept of overruling is rooted in the doctrine of precedent, where higher court decisions can alter or negate the legal standing of previous cases, making this task vital for ensuring that AI systems can assist in legal research and decision-making processes effectively.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 2400,
  "tags": [
    "LegalBench",
    "judicial opinion",
    "precedent",
    "rhetorical understanding",
    "rule application"
  ],
  "document_type": "judicial opinion",
  "min_input_length": 1,
  "max_input_length": 300,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[CaseHOLD](https://github.com/reglab/casehold)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Rhetorical-analysis",
  "task_type": "Binary classification"
}