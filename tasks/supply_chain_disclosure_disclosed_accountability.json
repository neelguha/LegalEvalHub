{
  "task_id": "supply_chain_disclosure_disclosed_accountability",
  "name": "supply_chain_disclosure_disclosed_accountability",
  "family": "LegalBench",
  "short_description": "Evaluates disclosure of accountability standards in supply chain statements.",
  "long_description": "The task measures the legal capability to interpret supply chain disclosures, specifically assessing whether these disclosures articulate the extent to which retail sellers or manufacturers have established internal accountability standards and procedures for addressing issues related to slavery and trafficking among their employees or contractors. This involves understanding the legal implications of such disclosures and the obligations companies may have under relevant laws and regulations aimed at combating human trafficking and ensuring ethical labor practices.\n\nLegal reasoning required for this task includes the ability to analyze and interpret the language used in supply chain disclosures, determining if they adequately address the accountability measures in place for non-compliance with company standards. This task is crucial for evaluating legal AI systems because it tests their ability to discern nuanced legal obligations and ethical considerations in corporate disclosures, which are increasingly important in today's regulatory environment. The background of this task is rooted in the legal frameworks surrounding corporate responsibility and human rights, highlighting the importance of transparency in supply chains to combat modern slavery and trafficking.\n",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 386,
  "tags": [
    "LegalBench",
    "accountability",
    "corporate law",
    "interpretation"
  ],
  "document_type": "disclosure statement",
  "min_input_length": 107,
  "max_input_length": 5764,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "Adam Chilton & Galit Sarfaty",
  "license": "[CC by 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}