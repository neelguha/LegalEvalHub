{
  "task_id": "cuad_minimum_commitment",
  "name": "cuad_minimum_commitment",
  "family": "LegalBench",
  "short_description": "Evaluates identification of minimum order commitments in contractual clauses.",
  "long_description": "The 'cuad_minimum_commitment' task measures the ability to identify contractual clauses that specify a minimum order size or minimum amount that one party must purchase from another. This task is crucial for understanding the obligations and commitments parties undertake in contractual agreements, which can significantly impact business operations and legal compliance. By accurately classifying these clauses, legal AI systems can assist in contract review processes, ensuring that parties are aware of their minimum purchasing obligations and can negotiate terms effectively.\n\nThis task requires legal reasoning and interpretation skills, as it involves analyzing the language of contractual clauses to determine whether they meet the criteria for minimum commitment. The ability to discern these specific commitments is essential for legal practitioners, as it can influence negotiations, enforceability, and the overall understanding of contractual obligations. Evaluating AI systems on this task is important because it ensures that they can effectively support legal professionals in identifying key terms that govern business relationships and contractual performance.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 778,
  "tags": [
    "LegalBench",
    "contract law",
    "interpretation",
    "minimum commitment",
    "rule application"
  ],
  "document_type": "contract clause",
  "min_input_length": 17,
  "max_input_length": 676,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/cuad>)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}