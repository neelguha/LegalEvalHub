{
  "task_id": "learned_hands_employment",
  "name": "learned_hands_employment",
  "family": "LegalBench",
  "short_description": "Classifies user posts for legal issues related to employment matters.",
  "long_description": "The 'learned_hands_employment' task evaluates the ability of legal AI systems to identify and classify user-generated content that implicates legal issues concerning employment. This includes a wide range of topics such as discrimination, harassment, workers' compensation, employee rights, union activities, wage disputes, pension issues, and wrongful termination. By focusing on these specific employment-related legal issues, the task assesses the AI's capability to recognize relevant legal concepts and their implications in real-world scenarios.\n\nThe task requires issue-spotting legal reasoning, where the AI must discern whether a given user post contains discussions that could lead to legal action or require legal advice. This type of analysis is crucial for developing AI systems that can assist individuals in navigating complex employment law matters, ensuring that they receive appropriate guidance based on the content of their inquiries. Evaluating AI performance on this task is vital for enhancing the reliability and accuracy of legal technology tools, ultimately contributing to better access to justice for individuals facing employment-related legal challenges.\n\nThe task is grounded in fundamental employment law principles, which govern the rights and responsibilities of both employers and employees. Understanding these principles is essential for the AI to effectively classify posts and provide meaningful insights or recommendations. By focusing on a diverse array of employment issues, this benchmark task plays a significant role in advancing the capabilities of legal AI systems in real-world applications.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 716,
  "tags": [
    "LegalBench",
    "discrimination",
    "employment law",
    "issue spotting",
    "issue-spotting"
  ],
  "document_type": "legal question",
  "min_input_length": 23,
  "max_input_length": 2327,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Learned Hands](https://spot.suffolklitlab.org/data/#learnedhands)",
  "license": "[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)",
  "legal_reasoning_type": "Issue-spotting",
  "task_type": "Binary classification"
}