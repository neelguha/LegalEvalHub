{
  "task_id": "cuad_no-solicit_of_employees",
  "name": "cuad_no-solicit_of_employees",
  "family": "LegalBench",
  "short_description": "Classifies clauses restricting employee solicitation in contracts.",
  "long_description": "The 'cuad_no-solicit_of_employees' task evaluates the legal capability to identify and classify contractual clauses that restrict a party from soliciting or hiring employees and/or contractors from the counterparty, both during and after the contract's duration. This task specifically measures the ability to interpret legal language and understand the implications of non-solicitation clauses within the context of contract law. It is essential for legal practitioners and AI systems to accurately recognize these clauses, as they play a critical role in protecting business interests and ensuring compliance with contractual obligations.\n\nLegal reasoning required for this task involves interpretation, as the model must discern the intent and scope of the clause in question. This includes analyzing the language used and determining whether it meets the criteria for a non-solicitation agreement. Evaluating such clauses is vital for legal AI systems, as it helps ensure that they can assist in contract review and compliance checks effectively. Understanding the nuances of non-solicitation clauses is important for businesses to mitigate risks related to employee poaching and to uphold the integrity of contractual relationships.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 148,
  "tags": [
    "LegalBench",
    "contract law",
    "interpretation",
    "non-solicitation"
  ],
  "document_type": "contract clause",
  "min_input_length": 15,
  "max_input_length": 475,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/cuad>)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}