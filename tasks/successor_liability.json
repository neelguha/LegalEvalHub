{
  "task_id": "successor_liability",
  "name": "successor_liability",
  "family": "LegalBench",
  "short_description": "Identify types of successor liability in asset purchase scenarios.",
  "long_description": "The 'successor_liability' task evaluates the understanding of the legal principles surrounding successor liability in corporate transactions, particularly when one company acquires the assets of another. This task measures the ability to recognize and classify different types of successor liability that may arise, including express agreement, fraudulent conveyance, de facto merger, and mere continuation. Legal practitioners must be adept at applying these doctrines to various fact patterns to determine potential liabilities that a purchaser may inherit from a seller's debts and obligations.\n\nTo successfully complete this task, legal reasoning and analytical skills are required, as participants must dissect complex fact patterns and apply relevant legal doctrines to arrive at a conclusion. This involves understanding not only the definitions and applications of each type of successor liability but also the nuances of how courts interpret these doctrines in practice. Evaluating this task is crucial for assessing the capabilities of legal AI systems, as it reflects their proficiency in navigating intricate legal concepts and their ability to provide accurate legal analyses in real-world scenarios. The task is grounded in common law principles that govern corporate asset transactions, making it essential for legal professionals to comprehend the implications of successor liability in their practice.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 50,
  "tags": [
    "LegalBench",
    "corporate law",
    "rule application",
    "rule conclusion",
    "successor liability"
  ],
  "document_type": "fact pattern",
  "min_input_length": 51,
  "max_input_length": 177,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "Frank Fagan",
  "license": "[CC by 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Application/conclusion",
  "task_type": "4-way classification"
}