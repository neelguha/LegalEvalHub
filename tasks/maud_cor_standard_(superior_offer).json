{
  "task_id": "maud_cor_standard_(superior_offer)",
  "name": "maud_cor_standard_(superior_offer)",
  "family": "LegalBench",
  "short_description": "Evaluates the standard for boards to follow regarding superior offers in merger agreements.",
  "long_description": "The 'maud_cor_standard_(superior_offer)' task measures the legal capability to interpret and apply standards of fiduciary duty within the context of merger agreements. Specifically, it assesses the ability to determine the appropriate standard a board must adhere to when considering whether to change its recommendation in light of a superior offer. This involves understanding the nuances of fiduciary duties and the implications of various legal standards that may arise in such scenarios.\n\nLegal reasoning required for this task includes interpretation of contractual language and the application of legal principles related to fiduciary duties. Participants must analyze excerpts from merger agreements and select the most accurate characterization of the board's obligations when faced with a superior offer. This task is crucial for evaluating legal AI systems as it tests their ability to navigate complex legal texts and make sound judgments based on established legal standards. The background knowledge of fiduciary duties is essential, as it pertains to the responsibilities of corporate boards to act in the best interests of shareholders, particularly during significant corporate transactions like mergers.\n",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 101,
  "tags": [
    "LegalBench",
    "corporate law",
    "fiduciary duty",
    "interpretation",
    "rule application"
  ],
  "document_type": "merger agreement",
  "min_input_length": 80,
  "max_input_length": 1630,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "10-way classification"
}