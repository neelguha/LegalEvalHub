{
  "task_id": "maud_buyer_consent_requirement_(ordinary_course)",
  "name": "maud_buyer_consent_requirement_(ordinary_course)",
  "family": "LegalBench",
  "short_description": "Evaluates the Buyer's consent limitations in merger agreements for ordinary operations.",
  "long_description": "The task measures the legal capability to interpret and analyze consent requirements within merger agreements, specifically focusing on the conditions under which a Buyer can withhold or delay consent for the acquired company's ordinary business operations. It assesses the understanding of contractual language and the implications of consent limitations, which are crucial in merger negotiations and compliance with legal standards.\n\nThis task requires legal reasoning that involves interpreting specific clauses in merger agreements and applying knowledge of corporate law principles. The analysis must consider the balance between the Buyer's rights and the operational needs of the acquired company, highlighting the importance of clear contractual terms to prevent disputes. Evaluating this task is vital for legal AI systems as it reflects their ability to navigate complex legal texts and provide accurate interpretations that can influence significant business decisions.\n\nUnderstanding the consent requirement in merger agreements is essential, as it can affect the operational continuity of the acquired entity and the overall success of the merger. The nuances of how consent can be conditioned, withheld, or delayed are critical for legal practitioners and AI systems alike, ensuring that they can effectively support legal decision-making processes in corporate transactions.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 182,
  "tags": [
    "LegalBench",
    "corporate law",
    "interpretation",
    "merger agreement"
  ],
  "document_type": "merger agreement",
  "min_input_length": 33,
  "max_input_length": 591,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}