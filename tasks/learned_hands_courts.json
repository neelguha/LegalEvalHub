{
  "task_id": "learned_hands_courts",
  "name": "learned_hands_courts",
  "family": "LegalBench",
  "short_description": "Classifies user posts related to legal issues involving courts.",
  "long_description": "The 'learned_hands_courts' task evaluates the capability of legal AI systems to identify and classify user-generated content that pertains to legal issues associated with courts. Specifically, it measures the ability to spot issues related to procedural matters, such as how individuals can interact with the legal system, including hiring lawyers, filing lawsuits, and representing oneself in court. This task requires a nuanced understanding of legal procedures and the ability to discern relevant legal topics from general discussions, making it a critical evaluation of an AI's legal reasoning capabilities.\n\nThe reasoning required for this task is primarily issue-spotting, where the AI must analyze the context of user posts to determine if they implicate legal issues. This involves recognizing keywords, phrases, and the overall context that indicate a discussion about court logistics. Evaluating AI systems through this task is vital as it reflects their practical application in real-world scenarios where individuals seek legal guidance. By ensuring that AI can accurately classify such posts, we can enhance its utility in providing accessible legal information and support to users navigating the complexities of the legal system.\n\nThis task is grounded in fundamental legal concepts related to court procedures and user interactions with the legal system. Understanding these concepts is essential for developing AI tools that can assist users effectively, ensuring that they receive accurate and relevant information when they need it most.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 198,
  "tags": [
    "LegalBench",
    "court procedures",
    "issue spotting",
    "issue-spotting",
    "legal issues"
  ],
  "document_type": "legal question",
  "min_input_length": 54,
  "max_input_length": 1235,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Learned Hands](https://spot.suffolklitlab.org/data/#learnedhands)",
  "license": "[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)",
  "legal_reasoning_type": "Issue-spotting",
  "task_type": "Binary classification"
}