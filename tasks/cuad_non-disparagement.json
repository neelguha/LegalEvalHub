{
  "task_id": "cuad_non-disparagement",
  "name": "cuad_non-disparagement",
  "family": "LegalBench",
  "short_description": "Classifies clauses requiring parties not to disparage each other.",
  "long_description": "The 'cuad_non-disparagement' task evaluates the ability to identify contractual clauses that impose a non-disparagement obligation on parties involved in an agreement. This task specifically measures the legal capability to interpret contract language and discern whether a clause explicitly restricts one party from making disparaging remarks about the other. It requires a nuanced understanding of legal terminology and the implications of non-disparagement clauses within the context of contract law.\n\nLegal reasoning in this task involves careful analysis and interpretation of the language used in contractual clauses. Participants must assess whether the wording of a clause aligns with the definition of non-disparagement, which typically prohibits parties from speaking negatively about each other in a manner that could harm reputations or business interests. This task is crucial for evaluating legal AI systems, as it tests their ability to accurately interpret and classify legal documents, which is essential for tasks such as contract review, compliance checks, and risk management in legal practice. Understanding non-disparagement clauses is important because they play a significant role in protecting the interests of parties in various agreements, including employment contracts, settlement agreements, and business partnerships.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 106,
  "tags": [
    "LegalBench",
    "contract law",
    "interpretation",
    "non-disparagement"
  ],
  "document_type": "contract clause",
  "min_input_length": 18,
  "max_input_length": 449,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/cuad>)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}