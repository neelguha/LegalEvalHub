{
  "task_id": "maud_definition_includes_stock_deals",
  "name": "maud_definition_includes_stock_deals",
  "family": "LegalBench",
  "short_description": "Evaluates understanding of superior offers in stock deals within merger agreements.",
  "long_description": "The task 'maud_definition_includes_stock_deals' measures the legal capability to interpret and analyze specific terms within merger agreements, particularly in relation to what constitutes a 'superior offer' in stock deals. It requires the evaluator to understand the nuances of contractual language and the implications of various thresholds for offers, which are critical in the context of mergers and acquisitions. This task assesses the ability to discern the correct interpretation of contractual clauses that can significantly impact the outcomes of business transactions.\n\nLegal reasoning in this task involves interpretation, as participants must analyze excerpts from merger agreements and apply their understanding of legal definitions and standards to select the most appropriate answer from multiple-choice options. This task is essential for evaluating legal AI systems because it tests their ability to comprehend complex legal documents and make informed decisions based on precise legal language. Understanding these concepts is vital for practitioners in corporate law, as they navigate the intricacies of merger agreements and ensure compliance with legal standards and best practices.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 149,
  "tags": [
    "LegalBench",
    "corporate law",
    "interpretation",
    "merger agreements"
  ],
  "document_type": "contract clause",
  "min_input_length": 90,
  "max_input_length": 1606,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "4-way classification"
}