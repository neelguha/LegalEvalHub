{
  "task_id": "learned_hands_education",
  "name": "learned_hands_education",
  "family": "LegalBench",
  "short_description": "Evaluates classification of legal issues related to education in user posts.",
  "long_description": "The 'learned_hands_education' task measures the ability of a legal AI system to identify and classify user-generated content that implicates legal issues within the realm of education. This includes topics such as accommodations for special needs students, discrimination in educational settings, student debt concerns, and disciplinary actions taken by schools. By focusing on these specific areas, the task assesses the AI's capability to recognize relevant legal concepts and issues that may require legal expertise or intervention.\n\nThe task requires issue-spotting legal reasoning, where the AI must analyze the context of user posts and determine whether they pertain to legal matters in education. This involves not only understanding the language used but also interpreting the nuances of educational law and policy. Evaluating this task is crucial for legal AI systems as it helps ensure that they can effectively assist users in navigating complex legal landscapes, particularly in sensitive areas like education, where legal implications can significantly impact individuals' lives. The background of this task is rooted in the need for accessible legal information and support for individuals facing educational challenges, making it a vital component of legal AI evaluation.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 62,
  "tags": [
    "LegalBench",
    "discrimination",
    "education law",
    "issue spotting",
    "issue-spotting"
  ],
  "document_type": "legal question",
  "min_input_length": 56,
  "max_input_length": 1181,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Learned Hands](https://spot.suffolklitlab.org/data/#learnedhands)",
  "license": "[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)",
  "legal_reasoning_type": "Issue-spotting",
  "task_type": "Binary classification"
}