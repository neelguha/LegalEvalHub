{
  "task_id": "maud_application_of_buyer_consent_requirement_(negative_interim_covenant)",
  "name": "maud_application_of_buyer_consent_requirement_(negative_interim_covenant)",
  "family": "LegalBench",
  "short_description": "Evaluates understanding of buyer consent in negative covenants of merger agreements.",
  "long_description": "This task measures the legal capability to interpret and analyze the application of buyer consent requirements within negative covenants in merger agreements. Specifically, it assesses the ability to discern whether the consent requirement applies to all negative covenants or only to specified ones, which is crucial for understanding the implications of such clauses in corporate transactions. The task requires nuanced legal reasoning and interpretation skills, as it involves evaluating contractual language and its potential effects on the parties involved in a merger.\n\nThe importance of this task lies in its relevance to legal AI systems that aim to assist practitioners in navigating complex merger agreements. By accurately identifying the scope of buyer consent in negative covenants, AI models can provide valuable insights that enhance decision-making processes in corporate law. This task is grounded in fundamental concepts of contract law, particularly regarding covenants and the obligations they impose on parties during mergers and acquisitions. Understanding these concepts is vital for legal professionals who must ensure compliance with contractual terms and mitigate risks associated with corporate transactions.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 181,
  "tags": [
    "LegalBench",
    "corporate law",
    "interpretation",
    "negative covenants",
    "rule application"
  ],
  "document_type": "contract clause",
  "min_input_length": 39,
  "max_input_length": 507,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/maud)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}