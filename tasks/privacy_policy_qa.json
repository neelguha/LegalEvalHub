{
  "task_id": "privacy_policy_qa",
  "name": "privacy_policy_qa",
  "family": "LegalBench",
  "short_description": "Evaluates if privacy policy clauses answer specific questions.",
  "long_description": "The 'privacy_policy_qa' task measures the capability of legal AI systems to interpret and analyze privacy policy clauses in relation to specific user inquiries. It assesses whether a given clause contains sufficient information to answer a question about data practices, such as data publication or user rights. This task requires an understanding of privacy law principles, the ability to extract relevant information, and the capacity to classify the relevance of the clause in a binary format—either as 'Relevant' or 'Irrelevant'.\n\nLegal reasoning in this context involves interpretation of the language used in privacy policies, which can often be complex and nuanced. The task is crucial for evaluating legal AI systems because it directly relates to how well these systems can assist users in understanding their rights and the obligations of organizations regarding personal data. Given the increasing importance of data privacy in today's digital landscape, this task provides insights into the effectiveness of AI in navigating legal texts and ensuring compliance with privacy regulations.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 10931,
  "tags": [
    "LegalBench",
    "data protection",
    "interpretation",
    "privacy law"
  ],
  "document_type": "privacy policy",
  "min_input_length": 11,
  "max_input_length": 240,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher Ré",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Privacy Q&A Corpus](https://github.com/AbhilashaRavichander/PrivacyQA_EMNLP)",
  "license": "[MIT](https://github.com/AbhilashaRavichander/PrivacyQA_EMNLP/blob/master/LICENSE)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}