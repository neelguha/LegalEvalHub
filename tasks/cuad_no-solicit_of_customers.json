{
  "task_id": "cuad_no-solicit_of_customers",
  "name": "cuad_no-solicit_of_customers",
  "family": "LegalBench",
  "short_description": "Evaluates clauses restricting solicitation of customers in contracts.",
  "long_description": "The 'cuad_no-solicit_of_customers' task measures the ability to identify and classify contractual clauses that impose restrictions on a party's ability to solicit customers or partners of the counterparty. This task specifically focuses on determining whether such restrictions apply during the term of the contract or extend beyond its termination. By analyzing these clauses, the task assesses the understanding of non-solicitation agreements, which are critical in protecting business interests and maintaining competitive advantages in various industries.\n\nLegal reasoning required for this task involves interpretation of contractual language and the implications of non-solicitation clauses. Participants must discern the nuances of the text to accurately classify whether a clause meets the criteria of restricting solicitation. This task is vital for evaluating legal AI systems, as it reflects the systems' capability to comprehend and analyze complex legal documents, ensuring compliance with contractual obligations and protecting against potential legal disputes. Understanding non-solicitation clauses is essential in contract law, as they play a significant role in safeguarding business relationships and intellectual property rights.",
  "dataset_url": "https://hazyresearch.stanford.edu/legalbench/",
  "num_samples": 90,
  "tags": [
    "LegalBench",
    "contract law",
    "interpretation",
    "non-solicitation",
    "rule application"
  ],
  "document_type": "contract clause",
  "min_input_length": 19,
  "max_input_length": 320,
  "metrics": [
    {
      "name": "accuracy",
      "direction": "maximize"
    },
    {
      "name": "balanced_accuracy",
      "direction": "maximize"
    },
    {
      "name": "f1_macro",
      "direction": "maximize"
    },
    {
      "name": "f1_micro",
      "direction": "maximize"
    },
    {
      "name": "valid_predictions_ratio",
      "direction": "maximize"
    }
  ],
  "contributed_by_name": "Neel Guha",
  "contributed_by_email": "nguha@cs.stanford.edu",
  "paper_url": "https://arxiv.org/abs/2308.11462",
  "paper_title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
  "paper_authors": [
    "Neel Guha",
    "Julian Nyarko",
    "Daniel E. Ho",
    "Christopher RÃ©",
    "Adam Chilton",
    "Aditya Narayana",
    "Alex Chohlas-Wood",
    "Austin Peters",
    "Brandon Waldon",
    "Daniel N. Rockmore",
    "Diego Zambrano",
    "Dmitry Talisman",
    "Enam Hoque",
    "Faiz Surani",
    "Frank Fagan",
    "Galit Sarfaty",
    "Gregory M. Dickinson",
    "Haggai Porat",
    "Jason Hegland",
    "Jessica Wu",
    "Joe Nudell",
    "Joel Niklaus",
    "John Nay",
    "Jonathan H. Choi",
    "Kevin Tobia",
    "Margaret Hagan",
    "Megan Ma",
    "Michael Livermore",
    "Nikon Rasumov-Rahe",
    "Nils Holzenberger",
    "Noam Kolt",
    "Peter Henderson",
    "Sean Rehaag",
    "Sharad Goel",
    "Shang Gao",
    "Spencer Williams",
    "Sunny Gandhi",
    "Tom Zur",
    "Varun Iyer",
    "Zehua Li"
  ],
  "source": "[Atticus Project](https://www.atticusprojectai.org/cuad>)",
  "license": "[CC By 4.0](https://creativecommons.org/licenses/by/4.0/)",
  "legal_reasoning_type": "Interpretation",
  "task_type": "Binary classification"
}